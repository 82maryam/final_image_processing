import cv2
import face_recognition
import os
import numpy as np
from sklearn.cluster import DBSCAN
from sklearn.decomposition import PCA
from scipy.spatial.distance import pdist
import matplotlib.pyplot as plt

# Ú©Ù„Ø§Ø³ Ø§ØµÙ„ÛŒ Ú©Ù‡ Ø§Ø³ØªØ®Ø±Ø§Ø¬ØŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©ÛŒÙÛŒØªØŒ Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ùˆ Ù†Ù…Ø§ÛŒØ´ Ú†Ù‡Ø±Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ¯Ù‡Ø¯
class EnhancedVideoFaceComparator:

    def __init__(self):
        # Ù„ÛŒØ³Øª Ú©Ø¯Ù‡Ø§ÛŒ Ø´Ù†Ø§Ø³Ù‡ (encodings) Ú†Ù‡Ø±Ù‡â€ŒÙ‡Ø§ÛŒ Ø´Ù†Ø§Ø®ØªÙ‡â€ŒØ´Ø¯Ù‡ (Ø¨Ø¹Ø¯ Ø§Ø² Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ)
        self.known_face_encodings = []
        # Ù„ÛŒØ³Øª Ø´Ù†Ø§Ø³Ù‡â€ŒÙ‡Ø§ÛŒ (IDs) ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ù‡Ø± ÙØ±Ø¯
        self.known_face_ids = []
        # Ø´Ù…Ø§Ø±Ù†Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø´Ù†Ø§Ø³Ù‡â€ŒÙ‡Ø§ÛŒ ÛŒÚ©ØªØ§
        self.face_counter = 0

    def apply_pca(self, encodings, variance_threshold=0.95):
        """
        Ø§Ø¹Ù…Ø§Ù„ PCA Ø±ÙˆÛŒ encodingÙ‡Ø§ÛŒ Ú†Ù‡Ø±Ù‡ Ø¨Ø±Ø§ÛŒ Ú©Ø§Ù‡Ø´ Ø§Ø¨Ø¹Ø§Ø¯
        """
        if len(encodings) <= 1:
            return encodings
        
        # Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
        encodings_standardized = (encodings - np.mean(encodings, axis=0)) / np.std(encodings, axis=0)
        
        # Ø§Ø¹Ù…Ø§Ù„ PCA
        pca = PCA(n_components=variance_threshold, random_state=42)
        encodings_pca = pca.fit_transform(encodings_standardized)
        
        print(f"ğŸ”¹ PCA: Reduced dimensions from {encodings.shape[1]} to {encodings_pca.shape[1]}")
        print(f"ğŸ”¹ Explained variance: {np.sum(pca.explained_variance_ratio_):.3f}")
        
        return encodings_pca

    def calculate_face_quality(self, face_image, face_location):
        """
        Ù…Ø­Ø§Ø³Ø¨Ù‡Ù” Ú©ÛŒÙÛŒØª ÛŒÚ© ØªØµÙˆÛŒØ± ØµÙˆØ±Øª Ø¨Ø§ Ú†Ù†Ø¯ Ø´Ø§Ø®Øµ Ø³Ø§Ø¯Ù‡:
        - Ø§Ù†Ø¯Ø§Ø²Ù‡ (size)
        - Ø±ÙˆØ´Ù†Ø§ÛŒÛŒ (brightness)
        - ÙˆØ¶ÙˆØ­/ØªÛŒØ²ÛŒ (sharpness)
        - Ù†Ø³Ø¨Øª Ø¹Ø±Ø¶/Ø§Ø±ØªÙØ§Ø¹ (aspect ratio)

        ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§:
        - face_image: ØªØµÙˆÛŒØ± Ø¨Ø±ÛŒØ¯Ù‡â€ŒØ´Ø¯Ù‡Ù” ØµÙˆØ±Øª (BGR)
        - face_location: Ù…Ú©Ø§Ù† ØµÙˆØ±Øª Ø¯Ø± ÙØ±ÛŒÙ… (Ø¨Ø±Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±Ø¯ Ø§Ø­ØªÙ…Ø§Ù„ÛŒ)

        Ø®Ø±ÙˆØ¬ÛŒ: Ø¹Ø¯Ø¯ÛŒ Ø¨ÛŒÙ† 0 Ùˆ 1 Ù†Ø´Ø§Ù†â€ŒØ¯Ù‡Ù†Ø¯Ù‡Ù” Ú©ÛŒÙÛŒØª (Ø¯Ø± Ø§ÛŒÙ† Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ù‚Ø¯Ø§Ø± Ø«Ø§Ø¨ØªÛŒ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯)
        """
        try:
            # Ø§Ù†Ø¯Ø§Ø²Ù‡ ØªØµÙˆÛŒØ± ØµÙˆØ±Øª (Ù¾ÛŒÚ©Ø³Ù„)
            height, width = face_image.shape[:2]

            # Ù†Ù…Ø±Ù‡Ù” Ø§Ù†Ø¯Ø§Ø²Ù‡: Ù†Ø³Ø¨Øª Ø§Ù†Ø¯Ø§Ø²Ù‡Ù” ØµÙˆØ±Øª Ø¨Ù‡ ÛŒÚ© Ø¢Ø³ØªØ§Ù†Ù‡Ù” 80x80ØŒ Ù…Ø­Ø¯ÙˆØ¯ Ø´Ø¯Ù‡ Ø¨Ù‡ 1.0
            size_score = min(height * width / (80 * 80), 1.0)

            # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ø®Ø§Ú©Ø³ØªØ±ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡Ù” Ø±ÙˆØ´Ù†Ø§ÛŒÛŒ Ùˆ ÙˆØ¶ÙˆØ­
            gray = cv2.cvtColor(face_image, cv2.COLOR_BGR2GRAY)

            # Ø±ÙˆØ´Ù†Ø§ÛŒÛŒ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù¾ÛŒÚ©Ø³Ù„â€ŒÙ‡Ø§ (Ù…ÛŒØ§Ù† 0 ØªØ§ 255)
            brightness = np.mean(gray)
            # Ù†Ù…Ø±Ù‡Ù” Ø±ÙˆØ´Ù†Ø§ÛŒÛŒ: Ù†Ø²Ø¯ÛŒÚ© Ø¨ÙˆØ¯Ù† Ø¨Ù‡ Ù…Ù‚Ø¯Ø§Ø± 127 (Ù…ÛŒØ§Ù†Ù‡) Ø¨Ù‡ØªØ± Ø§Ø³Øª
            brightness_score = 1.0 - abs(brightness - 127) / 127

            # ØªÛŒØ²ÛŒ ØªØµÙˆÛŒØ±: ÙˆØ§Ø±ÛŒØ§Ù†Ø³ Ù„Ø§Ù¾Ù„Ø§Ø³ÛŒØ§Ù† (Ù…Ù‚Ø¯Ø§Ø± Ø¨Ø§Ù„Ø§ØªØ± ÛŒØ¹Ù†ÛŒ ØªÛŒØ²ØªØ±)
            sharpness = cv2.Laplacian(gray, cv2.CV_64F).var()
            # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù†Ù…Ø±Ù‡Ù” ØªÛŒØ²ÛŒ Ø¨Ù‡ Ø¨Ø§Ø²Ù‡Ù” [0,1] Ø¨Ø§ ÛŒÚ© Ø¹Ø§Ù…Ù„ ØªÙ‚Ø³ÛŒÙ…
            sharpness_score = min(sharpness / 500, 1.0)

            # Ù†Ø³Ø¨Øª Ø¹Ø±Ø¶ Ø¨Ù‡ Ø§Ø±ØªÙØ§Ø¹ (Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ø¨Ø±Ø´ Ø®ÛŒÙ„ÛŒ Ú©Ø´ÛŒØ¯Ù‡ ÛŒØ§ Ù¾Ù‡Ù† Ù†Ø¨Ø§Ø´Ø¯)
            aspect_ratio = width / height if height > 0 else 1
            # Ù†Ù…Ø±Ù‡Ù” Ù†Ø³Ø¨Øª: ÙØ§ØµÙ„Ù‡ Ø§Ø² Ù†Ø³Ø¨Øª Ù…Ø·Ù„ÙˆØ¨ 0.8 Ú©Ù…â€ŒØªØ± Ø¨Ø§Ø´Ø¯ Ø¨Ù‡ØªØ± Ø§Ø³Øª
            aspect_score = 1.0 - min(abs(aspect_ratio - 0.8), 0.5)

            # ØªØ±Ú©ÛŒØ¨ ÙˆØ²Ù†ÛŒ Ù†Ù…Ø±Ù‡â€ŒÙ‡Ø§ (Ù‡Ø± ÙˆÛŒÚ˜Ú¯ÛŒ ÙˆØ²Ù†ÛŒ Ø¯Ø§Ø±Ø¯)
            actual_quality = (size_score * 0.3 + brightness_score * 0.2 +
                              sharpness_score * 0.3 + aspect_score * 0.2)

            # Ø¯Ø± Ú©Ø¯ Ø§ØµÙ„ÛŒ Ù…Ù‚Ø¯Ø§Ø± Ø«Ø§Ø¨Øª 0.9 Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡ (Ø¨Ø±Ø§ÛŒ ØªØ³Øª) â€” Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø² Ù…ÛŒâ€ŒØªÙˆØ§Ù† actual_quality Ø±Ø§ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯
            # fixed_quality = 0.9
            # return actual_quality  # Ø§Ú¯Ø± Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒØ¯ Ú©ÛŒÙÛŒØª ÙˆØ§Ù‚Ø¹ÛŒ Ø±Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯
            return actual_quality

        except Exception as e:
            # Ø¯Ø± ØµÙˆØ±Øª Ø¨Ø±ÙˆØ² Ø®Ø·Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ù…Ø­Ø§ÙØ¸Ù‡â€ŒÚ©Ø§Ø±Ø§Ù†Ù‡ Ù…Ù‚Ø¯Ø§Ø± 0.8 Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†ÛŒÙ…
            return 0.8

    def extract_and_cluster_faces(self, video_path, output_dir="faces", max_faces=50, frame_interval=10):
        """
        Ø§ÛŒÙ† ØªØ§Ø¨Ø¹:
        1. ÙˆÛŒØ¯ÛŒÙˆ Ø±Ø§ Ø¨Ø§Ø² Ù…ÛŒâ€ŒÚ©Ù†Ø¯
        2. ÙØ±ÛŒÙ…â€ŒÙ‡Ø§ Ø±Ø§ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù†Ø¯ (Ù‡Ø± frame_interval ÛŒÚ©Ø¨Ø§Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´)
        3. Ú†Ù‡Ø±Ù‡â€ŒÙ‡Ø§ Ø±Ø§ ØªØ´Ø®ÛŒØµ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ùˆ encoding Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯
        4. Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú†Ù‡Ø±Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
        5. Ø¯Ø± Ù¾Ø§ÛŒØ§Ù† Ø¨Ø§ ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ cluster_faces_and_select_best Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ùˆ Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ØªØ±ÛŒÙ† Ú†Ù‡Ø±Ù‡ Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ¯Ù‡Ø¯

        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        - video_path: Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ ÙˆÛŒØ¯ÛŒÙˆ
        - output_dir: Ù¾ÙˆØ´Ù‡â€ŒØ§ÛŒ Ú©Ù‡ Ø¹Ú©Ø³â€ŒÙ‡Ø§ÛŒ Ù…Ù†ØªØ®Ø¨ Ø¯Ø± Ø¢Ù† Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯
        - max_faces: Ø­Ø¯Ø§Ú©Ø«Ø± ØªØ¹Ø¯Ø§Ø¯ Ú†Ù‡Ø±Ù‡â€ŒØ§ÛŒ Ú©Ù‡ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù†ÛŒÙ… (Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ø¨Ø±Ø§ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯)
        - frame_interval: Ù‡Ø± Ú†Ù†Ø¯ ÙØ±ÛŒÙ… ÛŒÚ©â€ŒØ¨Ø§Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø§Ù†Ø¬Ø§Ù… Ø´ÙˆØ¯
        """
        # Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡ Ø®Ø±ÙˆØ¬ÛŒ Ø¯Ø± ØµÙˆØ±Øª Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

        # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ ÙØ§ÛŒÙ„ ÙˆÛŒØ¯ÛŒÙˆ
        if not os.path.exists(video_path):
            print(f" Video not found: {video_path}")
            return []

        # Ø¨Ø§Ø² Ú©Ø±Ø¯Ù† ÙˆÛŒØ¯ÛŒÙˆ Ø¨Ø§ OpenCV
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            print(f"âŒ Error opening video: {video_path}")
            return []

        all_face_data = []  # Ù„ÛŒØ³Øª Ø¨Ø±Ø§ÛŒ Ù†Ú¯Ù‡Ø¯Ø§Ø±ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª ØªÙ…Ø§Ù… Ú†Ù‡Ø±Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬â€ŒØ´Ø¯Ù‡
        frame_count = 0
        print(f"ğŸ¥ Processing video: {video_path}")

        # Ø­Ù„Ù‚Ù‡Ù” Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ±ÛŒÙ…â€ŒÙ‡Ø§
        while cap.isOpened() and len(all_face_data) < max_faces:
            ret, frame = cap.read()
            if not ret:
                # Ø§Ú¯Ø± ÙØ±ÛŒÙ… Ø®ÙˆØ§Ù†Ø¯Ù‡ Ù†Ø´Ø¯ => Ù¾Ø§ÛŒØ§Ù† ÙˆÛŒØ¯ÛŒÙˆ
                break

            # ÙÙ‚Ø· Ù‡Ø± frame_interval ÙØ±ÛŒÙ… Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÛŒâ€ŒØ´ÙˆØ¯ ØªØ§ Ø³Ø±Ø¹Øª Ø§ÙØ²Ø§ÛŒØ´ ÛŒØ§Ø¨Ø¯
            if frame_count % frame_interval == 0:
                try:
                    # ØªØ¨Ø¯ÛŒÙ„ BGR->RGB Ú†ÙˆÙ† face_recognition Ø§Ø² RGB Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
                    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

                    # ØªØ´Ø®ÛŒØµ Ù…ÙˆÙ‚Ø¹ÛŒØª Ú†Ù‡Ø±Ù‡â€ŒÙ‡Ø§ (Ù„ÛŒØ³Øª Ø§Ø² (top, right, bottom, left))
                    face_locations = face_recognition.face_locations(rgb_frame, model="hog")

                    if face_locations:
                        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ encoding Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú†Ù‡Ø±Ù‡
                        face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)
                        # Ù¾ÛŒÙ…Ø§ÛŒØ´ Ù‡Ù…Ø²Ù…Ø§Ù† encoding Ùˆ Ù…Ú©Ø§Ù†â€ŒÙ‡Ø§
                        for encoding, (top, right, bottom, left) in zip(face_encodings, face_locations):

                            # Ø§ÛŒÙ…Ù†â€ŒØ³Ø§Ø²ÛŒ Ù…Ø­Ø¯ÙˆØ¯Ù‡â€ŒÙ‡Ø§ ØªØ§ Ø§Ø² Ø®Ø±ÙˆØ¬ Ø§Ø² Ø§Ù†Ø¯Ø§Ø²Ù‡Ù” ØªØµÙˆÛŒØ± Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø´ÙˆØ¯
                            top, bottom = max(0, top), min(frame.shape[0], bottom)
                            left, right = max(0, left), min(frame.shape[1], right)

                            # Ø§Ú¯Ø± Ù…Ø­Ø¯ÙˆØ¯Ù‡Ù” Ø¨Ø±Ø´ Ù…Ø¹Ú©ÙˆØ³ Ø¨ÙˆØ¯ Ø±Ø¯ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
                            if bottom <= top or right <= left:
                                continue

                            # Ø¨Ø±Ø´ ØªØµÙˆÛŒØ± ØµÙˆØ±Øª Ø§Ø² ÙØ±ÛŒÙ… Ø§ØµÙ„ÛŒ (BGR)
                            face_image = frame[top:bottom, left:right]

                            # Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ† ØµÙˆØ±Øªâ€ŒÙ‡Ø§ÛŒ Ø®ÛŒÙ„ÛŒ Ú©ÙˆÚ†Ú©
                            if face_image.shape[0] < 50 or face_image.shape[1] < 50:
                                continue

                            # Ù…Ø­Ø§Ø³Ø¨Ù‡Ù” Ú©ÛŒÙÛŒØª Ø¨Ø±Ø§ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ØªØ±ÛŒÙ† ØªØµÙˆÛŒØ± Ø¯Ø± Ø®ÙˆØ´Ù‡
                            quality = self.calculate_face_quality(face_image, (top, right, bottom, left))

                            # Ø°Ø®ÛŒØ±Ù‡Ù” Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú†Ù‡Ø±Ù‡ Ø¯Ø± Ù„ÛŒØ³Øª
                            all_face_data.append({
                                'encoding': encoding,
                                'location': (top, right, bottom, left),
                                'image': face_image,
                                'quality_score': quality,
                                'frame_id': frame_count
                            })

                except Exception as e:
                    # Ø¯Ø± ØµÙˆØ±Øª Ø®Ø·Ø§ Ø¨Ø±Ø§ÛŒ Ø¢Ù† ÙØ±ÛŒÙ… ÙÙ‚Ø· Ù¾ÛŒØºØ§Ù… Ù‡Ø´Ø¯Ø§Ø± Ú†Ø§Ù¾ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ùˆ Ø§Ø¯Ø§Ù…Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ…
                    print(f"âš ï¸ Error in frame {frame_count}: {e}")

            # Ø§ÙØ²Ø§ÛŒØ´ Ø´Ù…Ø§Ø±Ù†Ø¯Ù‡Ù” ÙØ±ÛŒÙ…
            frame_count += 1

        # Ø¢Ø²Ø§Ø¯Ø³Ø§Ø²ÛŒ Ù…Ù†Ø¨Ø¹ ÙˆÛŒØ¯ÛŒÙˆ
        cap.release()
        print(f"âœ… {len(all_face_data)} faces extracted.")

        if not all_face_data:
            print("No faces found.")
            return []

        # ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ùˆ Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ØªØ±ÛŒÙ†â€ŒÙ‡Ø§
        return self.cluster_faces_and_select_best(all_face_data, output_dir)

    def cluster_faces_and_select_best(self, all_face_data, output_dir):
        """
        1. Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ encodeÙ‡Ø§ Ø¨Ø§ DBSCAN Ùˆ PCA
        2. Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø®ÙˆØ´Ù‡ (Ù‡Ø± Ø´Ø®Øµ) Ø¨Ù‡ØªØ±ÛŒÙ† ØªØµÙˆÛŒØ± Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ quality_score Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
        3. Ø¹Ú©Ø³â€ŒÙ‡Ø§ÛŒ Ù…Ù†ØªØ®Ø¨ Ø±Ø§ Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ùˆ Ø´Ù†Ø§Ø³Ù‡Ù” ÛŒÚ©ØªØ§ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒÙ†Ù…Ø§ÛŒØ¯
        """
        # ØªØ¨Ø¯ÛŒÙ„ Ù„ÛŒØ³Øª encodingÙ‡Ø§ Ø¨Ù‡ Ø¢Ø±Ø§ÛŒÙ‡Ù” numpy Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ø§Øª
        encodings = np.array([face['encoding'] for face in all_face_data])

        # ğŸ”¥ Ù†Ù…Ø§ÛŒØ´ Ú†Ù‡Ø±Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ Ú©ÛŒÙÛŒØª Ø¨Ø§Ù„Ø§ÛŒ 70% Ù‚Ø¨Ù„ Ø§Ø² Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ
        high_quality_faces = [face for face in all_face_data if face['quality_score'] >= 0.7]
        print(f"ğŸ”¹ Faces with quality >= 70%: {len(high_quality_faces)}/{len(all_face_data)}")
        
        # Ù†Ù…Ø§ÛŒØ´ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú†Ù‡Ø±Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ Ú©ÛŒÙÛŒØª Ø¨Ø§Ù„Ø§
        for i, face in enumerate(high_quality_faces):
            print(f"   ğŸ‘¤ High Quality Face {i+1}: Score = {face['quality_score']:.3f}, Frame = {face['frame_id']}")

        # Ø§Ø¹Ù…Ø§Ù„ PCA Ø¨Ø±Ø§ÛŒ Ú©Ø§Ù‡Ø´ Ø§Ø¨Ø¹Ø§Ø¯
        if len(encodings) > 1:
            encodings_for_clustering = self.apply_pca(encodings)
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡Ù” Ù¾ÙˆÛŒØ§ Ø¨Ø±Ø§ÛŒ eps Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ù‡Ø´ Ø¨Ø¹Ø¯ ÛŒØ§ÙØªÙ‡
            distances = pdist(encodings_for_clustering, 'euclidean')
            mean_distance = np.mean(distances)
            std_distance = np.std(distances)
            
            # ØªÙ†Ø¸ÛŒÙ… eps Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ PCA Ø´Ø¯Ù‡
            eps_value = min(0.6, max(0.4, mean_distance + 0.5 * std_distance))
        else:
            # Ø§Ú¯Ø± ÙÙ‚Ø· ÛŒÚ© Ú†Ù‡Ø±Ù‡ Ø¯Ø§Ø±ÛŒÙ…ØŒ Ø§Ø² PCA Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ú©Ù†ÛŒÙ…
            encodings_for_clustering = encodings
            eps_value = 0.4

        print(f"ğŸ”¹ Adaptive eps value = {eps_value:.3f}")

        # Ø§Ø¬Ø±Ø§ÛŒ DBSCAN Ø±ÙˆÛŒ encodings Ú©Ø§Ù‡Ø´ Ø¨Ø¹Ø¯ ÛŒØ§ÙØªÙ‡
        clustering = DBSCAN(eps=eps_value, min_samples=1, metric='euclidean').fit(encodings_for_clustering)
        labels = clustering.labels_  # Ù„ÛŒØ¨Ù„ Ù‡Ø± Ù†Ù…ÙˆÙ†Ù‡ (Ù‡Ø± Ø¨Ø±Ú†Ø³Ø¨ ÛŒÚ© Ø®ÙˆØ´Ù‡ Ø§Ø³ØªØ› -1 ÛŒØ¹Ù†ÛŒ noise)

        # Ú¯Ø±ÙˆÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¨Ø±Ú†Ø³Ø¨
        clusters = {}
        for i, label in enumerate(labels):
            clusters.setdefault(label, []).append(all_face_data[i])

        # ØªØ¹Ø¯Ø§Ø¯ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§ÛŒ ØºÛŒØ±-Ù†ÙˆÛŒØ²
        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
        print(f"ğŸ“Š Clustering result: {n_clusters} clusters, {list(labels).count(-1)} noise points")

        # Ø§Ú¯Ø± Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒÛŒ Ø¨Ø§ Ø¨Ø±Ú†Ø³Ø¨ -1 (Ù†ÙˆÛŒØ²) ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´ØªÙ†Ø¯: Ø³Ø¹ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ø¢Ù†â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ù‡ Ù†Ø²Ø¯ÛŒÚ©â€ŒØªØ±ÛŒÙ† Ø®ÙˆØ´Ù‡ Ø§Ø®ØªØµØ§Øµ Ø¯Ù‡ÛŒÙ…
        noise_faces = clusters.pop(-1, [])
        for noise_face in noise_faces:
            best_match_id = None
            best_distance = 1.0 # Ø­Ø¯Ø§Ú©Ø«Ø± ÙØ§ØµÙ„Ù‡ Ù…Ù…Ú©Ù† Ø¨Ø±Ø§ÛŒ encodings Ø¯Ø± Ø§ÛŒÙ† Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø­Ø¯ÙˆØ¯ 1.0 Ø§Ø³Øª

            # Ø¨Ø±Ø±Ø³ÛŒ ÙØ§ØµÙ„Ù‡ Ù†ÙˆÛŒØ² Ø¨Ù‡ ØªÙ…Ø§Ù… Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ùˆ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ú©Ù…ØªØ±ÛŒÙ† ÙØ§ØµÙ„Ù‡
            for cluster_id, faces in clusters.items():
                cluster_encs = [f['encoding'] for f in faces]
                distances = face_recognition.face_distance(cluster_encs, noise_face['encoding'])
                min_dist = np.min(distances)

                if min_dist < best_distance:
                    best_distance = min_dist
                    best_match_id = cluster_id

            # Ø§Ú¯Ø± Ù†ÙˆÛŒØ² Ø¨Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡Ù” Ú©Ø§ÙÛŒ Ù†Ø²Ø¯ÛŒÚ© Ø¨Ù‡ ÛŒÚ© Ø®ÙˆØ´Ù‡ Ø¨ÙˆØ¯ (Ø¢Ø³ØªØ§Ù†Ù‡ 0.45) Ø¢Ù† Ø±Ø§ Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
            if best_distance < 0.45 and best_match_id is not None:
                clusters[best_match_id].append(noise_face)

        best_faces, best_encodings, best_ids = [], [], []

        # Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø®ÙˆØ´Ù‡Ø› Ø¨Ù‡ØªØ±ÛŒÙ† ØªØµÙˆÛŒØ± Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ quality_score Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
        for cluster_id, faces in clusters.items():
            # Ø§Ù†ØªØ®Ø§Ø¨ Ø¹Ù†ØµØ± Ø¨Ø§ Ø¨ÛŒØ´ØªØ±ÛŒÙ† Ù†Ù…Ø±Ù‡Ù” Ú©ÛŒÙÛŒØª
            best_face = max(faces, key=lambda x: x['quality_score'])

            # Ø³Ø§Ø®Øª ÛŒÚ© Ø´Ù†Ø§Ø³Ù‡Ù” ÛŒÚ©ØªØ§ Ø¨Ø±Ø§ÛŒ Ù‡Ø± ÙØ±Ø¯ Ø¨Ø±Ø§Ø³Ø§Ø³ Ø´Ù…Ø§Ø±Ù†Ø¯Ù‡ Ùˆ Ù†Ù…Ø±Ù‡Ù” Ú©ÛŒÙÛŒØª
            face_id = f"person_{self.face_counter+1:03d}_q{best_face['quality_score']:.2f}"
            best_faces.append(best_face)
            best_encodings.append(best_face['encoding'])
            best_ids.append(face_id)

            # Ø°Ø®ÛŒØ±Ù‡Ù” Ø¹Ú©Ø³ Ø¨Ù‡ØªØ±ÛŒÙ† Ú†Ù‡Ø±Ù‡ Ø¨Ø±Ø§ÛŒ Ø¢Ù† Ø®ÙˆØ´Ù‡
            cv2.imwrite(f"{output_dir}/{face_id}.jpg", best_face['image'])
            print(f"ğŸ‘¤ Cluster {cluster_id}: {len(faces)} faces â†’ Best Quality: {best_face['quality_score']:.3f}")
            self.face_counter += 1

        # ğŸ”¥ Ù†Ù…Ø§ÛŒØ´ Ú†Ù‡Ø±Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø§ Ú©ÛŒÙÛŒØª Ø¨Ø§Ù„Ø§ÛŒ 70%
        high_quality_final = [face for face in best_faces if face['quality_score'] >= 0.7]
        print(f"\nğŸ¯ FINAL - High quality faces (>=70%): {len(high_quality_final)}/{len(best_faces)}")
        for i, face in enumerate(high_quality_final):
            print(f"   âœ… Person {i+1}: Quality = {face['quality_score']:.3f}")

        # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù„ÛŒØ³Øªâ€ŒÙ‡Ø§ÛŒ Ø´Ù†Ø§Ø®ØªÙ‡â€ŒØ´Ø¯Ù‡ Ø¯Ø± Ø´ÛŒØ¡
        self.known_face_encodings = best_encodings
        self.known_face_ids = best_ids
        return best_faces

    def display_best_faces(self, faces_data, title="Unique Identified Faces"):
        """
        Ù†Ù…Ø§ÛŒØ´ ØªØµÙˆÛŒØ±ÛŒ Ø¨Ù‡ØªØ±ÛŒÙ† Ú†Ù‡Ø±Ù‡Ù” Ù‡Ø± ÙØ±Ø¯ Ø¨Ø§ matplotlib
        """
        if not faces_data:
            print("No faces to display.")
            return

        n = len(faces_data)
        cols = min(3, n)  # ØªØ¹Ø¯Ø§Ø¯ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ Ø¯Ø± Ù†Ù…Ø§ (Ø­Ø¯Ø§Ú©Ø«Ø± 3)
        rows = (n + cols - 1) // cols

        plt.figure(figsize=(15, 5 * rows))
        plt.suptitle(f"{title}\nNumber of Unique Individuals: {n}", fontsize=16)

        for i, face_data in enumerate(faces_data):
            plt.subplot(rows, cols, i + 1)
            # ØªØ¨Ø¯ÛŒÙ„ BGR->RGB Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ ØµØ­ÛŒØ­ Ø¯Ø± matplotlib
            face_rgb = cv2.cvtColor(face_data['image'], cv2.COLOR_BGR2RGB)
            plt.imshow(face_rgb)
            plt.title(f"Person {i+1}\nQuality: {face_data['quality_score']:.2f}")
            plt.axis('off')

        plt.tight_layout()
        plt.show()

    def display_high_quality_faces(self, faces_data, quality_threshold=0.7):
        """
        Ù†Ù…Ø§ÛŒØ´ Ú†Ù‡Ø±Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ Ú©ÛŒÙÛŒØª Ø¨Ø§Ù„Ø§
        """
        high_quality_faces = [face for face in faces_data if face['quality_score'] >= quality_threshold]
        
        if not high_quality_faces:
            print(f"ğŸ”¹ No faces with quality >= {quality_threshold*100}%")
            return

        n = len(high_quality_faces)
        cols = min(3, n)
        rows = (n + cols - 1) // cols

        plt.figure(figsize=(15, 5 * rows))
        plt.suptitle(f"High Quality Faces (Quality >= {quality_threshold*100}%)\nTotal: {n} faces", fontsize=16)

        for i, face_data in enumerate(high_quality_faces):
            plt.subplot(rows, cols, i + 1)
            face_rgb = cv2.cvtColor(face_data['image'], cv2.COLOR_BGR2RGB)
            plt.imshow(face_rgb)
            plt.title(f"Quality: {face_data['quality_score']:.2f}\nFrame: {face_data['frame_id']}")
            plt.axis('off')

        plt.tight_layout()
        plt.show()


# ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡ Ú©Ù‡ Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒØ´ÙˆØ¯
def main():
    comparator = EnhancedVideoFaceComparator()
    print("=" * 50)
    print("1=boy\n2=BOY2\n3=two girl")
    a = input("Choose video: ")

    video_map = {
        '1': "user_72959_video_5.mp4",
        '2': "user_93151_video_17.mp4",
        '3': "user_94842_video_0.mp4"
    }

    video_path = video_map.get(a)

    if not video_path or not os.path.exists(video_path):
        print(f"âŒ Video file not found: {video_path}")
        return

    print("ğŸš€ Starting face extraction and clustering with PCA...")
    faces = comparator.extract_and_cluster_faces(
        video_path,
        output_dir="reference_faces",
        frame_interval=15,
        max_faces=50
    )

    if faces:
        print(f"\nğŸ‰ Success! {len(faces)} unique individuals identified.\n")
        for i, f in enumerate(faces):
            print(f"Person {i+1}: Quality = {f['quality_score']:.2f}, Frame = {f['frame_id']}")
        
        # Ù†Ù…Ø§ÛŒØ´ Ù†ØªØ§ÛŒØ¬ Ø§ØµÙ„ÛŒ
        comparator.display_best_faces(faces)
        
        # ğŸ”¥ Ù†Ù…Ø§ÛŒØ´ Ú†Ù‡Ø±Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ Ú©ÛŒÙÛŒØª Ø¨Ø§Ù„Ø§
        print("\n" + "="*50)
        print("ğŸ”¼ HIGH QUALITY FACES DISPLAY")
        comparator.display_high_quality_faces(faces, quality_threshold=0.7)
        
    else:
        print("No unique faces were identified.")


if __name__ == "__main__":
    main()